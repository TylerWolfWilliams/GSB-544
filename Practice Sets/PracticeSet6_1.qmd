---
title: Practice Set 6.1
echo: true
fig-height: 3.5
fig-width: 6
execute:
    warning: false
format:
  html:
    code-fold: true
    embed-resources: true
---
GitHub link: https://github.com/TylerWolfWilliams/GSB-544

# Palmer Penguins Modeling

Import the Palmer Penguins dataset and print out the first few rows.

Suppose we want to predict `bill_depth_mm` using the other variables in the dataset.

Which variables would we need to **dummify**?

# Import Libraries
```{python}
from palmerpenguins import load_penguins
import sklearn
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
```

Let's use `bill_length_mm` to predict `bill_depth_mm`. Prepare your data and fit the following models on the entire dataset:

* Simple linear regression (e.g. straight-line) model
* Quadratic (degree 2 polynomial) model
* Cubic (degree 3 polynomial) model
* Degree 10 polynomial model

Make predictions for each model and plot your fitted models on the scatterplot.
# Load in Penguins
```{python}
df = load_penguins()
df.head()
```
# Data Setup
```{python}
df = df.dropna(subset=["bill_length_mm", "bill_depth_mm"])

y = df['bill_depth_mm']
X = df[['bill_length_mm']]
```
# Test Train Split
```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
```
# FItting Linear Model
```{python}
model_lin = LinearRegression()
model_lin.fit(X_train, y_train)

y_pred_lin = model_lin.predict(X_test)
y_train_pred_lin = model_lin.predict(X_train)
```
# Plotting Linear Model
```{python}
plt.scatter(X_test, y_pred_lin, color="gray", alpha=0.6, label="Data")
plt.show()
```
# Cleaning Data For Quadratic
```{python}
df = df.dropna(subset=["bill_length_mm", "bill_depth_mm"])

y = df['bill_depth_mm']
X = df[['bill_length_mm']]

X.columns = ["x"]
X["x_sq"] = X["x"]**2
X["x_cube"] = X["x"]**3

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
```
# Fitting Quadratic Model
```{python}
model_quad = LinearRegression()
model_quad_fitted = model_quad.fit(X_train[["x", "x_sq"]], y_train)

y_pred_quad = model_quad_fitted.predict(X_test[["x", "x_sq"]])
```
# Plotting Quadratic Model
```{python}
plt.scatter(X_test["x"], y_pred_quad, color="gray", alpha=0.6, label="Data")
plt.show()
```
# Fitting Cubic Model
```{python}
model_cube= LinearRegression()
model_cube_fitted = model_cube.fit(X_train[["x", "x_sq", "x_cube"]], y_train)

y_pred_cube = model_cube_fitted.predict(X_test[["x", "x_sq", "x_cube"]])
```
# Plot Cubic Model
```{python}
plt.scatter(X_test["x"], y_pred_cube, color="gray", alpha=0.6, label="Data")
plt.show()
```
# Setup for 10 Degree Model
```{python}
df = df.dropna(subset=["bill_length_mm", "bill_depth_mm"])

y = df['bill_depth_mm']
X = df[['bill_length_mm']]

X.columns = ["x"]
X["x_sq"] = X["x"]**2
X["x_cube"] = X["x"]**3
X["x_four"] = X["x"]**4
X["x_five"] = X["x"]**5
X["x_six"] = X["x"]**6
X["x_seven"] = X["x"]**7
X["x_eight"] = X["x"]**8
X["x_nine"] = X["x"]**9
X["x_ten"] = X["x"]**10

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
```
# 10 Degree Model
```{python}
model_ten = LinearRegression()
model_ten_fitted = model_ten.fit(X_train[["x", "x_sq", "x_cube", "x_four", "x_five", "x_six", "x_seven", "x_eight", "x_nine", "x_ten"]], y_train)

y_pred_ten = model_ten_fitted.predict(X_test[["x", "x_sq", "x_cube", "x_four", "x_five", "x_six", "x_seven", "x_eight", "x_nine", "x_ten"]])
```
# Plotting 10 degree model
```{python}
plt.scatter(X_test["x"], y_pred_ten, color="gray", alpha=0.6, label="Data")
plt.show()
```
# Plotting X, y
```{python}
plt.scatter(X['x'], y, color="gray", alpha=0.6, label="Data")
plt.show()
```
# Getting training data predictions
```{python}
y_train_pred_quad = model_quad_fitted.predict(X_train[["x", "x_sq"]])
y_train_pred_cube = model_cube_fitted.predict(X_train[["x", "x_sq", "x_cube"]])
y_train_pred_ten = model_ten_fitted.predict(X_train[["x", "x_sq", "x_cube", "x_four", "x_five", "x_six", "x_seven", "x_eight", "x_nine", "x_ten"]])
```
# RMSE Results
```{python}
rmse_lin_train = np.sqrt(mean_squared_error(y_train, y_train_pred_lin))
rmse_lin_test  = np.sqrt(mean_squared_error(y_test,  y_pred_lin))

rmse_quad_train = np.sqrt(mean_squared_error(y_train, y_train_pred_quad))
rmse_quad_test  = np.sqrt(mean_squared_error(y_test,  y_pred_quad))

rmse_cube_train = np.sqrt(mean_squared_error(y_train, y_train_pred_cube))
rmse_cube_test  = np.sqrt(mean_squared_error(y_test,  y_pred_cube))

rmse_ten_train = np.sqrt(mean_squared_error(y_train, y_train_pred_ten))
rmse_ten_test  = np.sqrt(mean_squared_error(y_test,  y_pred_ten))

print("Linear:  Train RMSE =", round(rmse_lin_train, 3),  "| Test RMSE =", round(rmse_lin_test, 3))
print("Quad:    Train RMSE =", round(rmse_quad_train, 3), "| Test RMSE =", round(rmse_quad_test, 3))
print("Cubic:   Train RMSE =", round(rmse_cube_train, 3), "| Test RMSE =", round(rmse_cube_test, 3))
print("Ten: Train RMSE =", round(rmse_ten_train, 3), "| Test RMSE =", round(rmse_ten_test, 3))
```

* Are any of the models above underfitting the data? If so, which ones and how can you tell?
* Are any of thhe models above overfitting the data? If so, which ones and how can you tell?
* Which of the above models do you think fits the data best and why?

None of the models appear to be overfit, The 10 degree model has the lowest RMSE for the testing data which would not be true if it were overfit. The linear, quadratic, and cubic models may be under fit as their test RMSE values are all higher than the 10 degree model.