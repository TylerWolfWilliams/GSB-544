---
title: Practice Set 6.2
echo: true
fig-height: 3.5
fig-width: 6
execute:
    warning: false
format:
  html:
    code-fold: true
    embed-resources: true
---
GitHub link: https://github.com/TylerWolfWilliams/GSB-544

# Palmer Penguins Modeling

Import the Palmer Penguins dataset and print out the first few rows.

Suppose we want to predict `bill_depth_mm` using the other variables in the dataset.

**Dummify** all variables that require this.

```{python}
from palmerpenguins import load_penguins
import sklearn
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
```

```{python}
df = load_penguins()
df = df.dropna()
df.head()
```

```{python}
df_encoded = pd.get_dummies(df, columns=['species', 'island', 'sex'], drop_first=True)
df_encoded.head()
```

Let's use the other variables to predict `bill_depth_mm`. Prepare your data and fit the following models on a training dataset subset of the entire dataset:

* Four different models, each containing a different set of predictor variables

Create a plot like the right plot of Fig 1. in our `Model Validation` chapter with the training and test error plotted for each of your four models.

Which of your models was best?

```{python}
y = df_encoded['bill_depth_mm']
X1 = df_encoded[['bill_length_mm', 'sex_male']]
X2 = df_encoded[['species_Chinstrap', 'species_Gentoo', 'body_mass_g']]
X3 = df_encoded[['island_Dream', 'island_Torgersen', 'flipper_length_mm']]
X4 = df_encoded.drop(columns=['bill_depth_mm'])

column_sets = [X1, X2, X3, X4]
num_features = np.array([X1.shape[1], X2.shape[1], X3.shape[1], X4.shape[1]])
```

```{python}
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size=0.25)
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.25)
X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y, test_size=0.25)
X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y, test_size=0.25)

model = LinearRegression()

model.fit(X_train1, y_train1)
y_pred1 = model.predict(X_test1)
y_train_pred1 = model.predict(X_train1)
model.fit(X_train2, y_train2)
y_pred2 = model.predict(X_test2)
y_train_pred2 = model.predict(X_train2)
model.fit(X_train3, y_train3)
y_pred3 = model.predict(X_test3)
y_train_pred3 = model.predict(X_train3)
model.fit(X_train4, y_train4)
y_pred4 = model.predict(X_test4)
y_train_pred4 = model.predict(X_train4)

mse_train1 = mean_squared_error(y_train1, y_train_pred1)
mse_test1  = mean_squared_error(y_test1,  y_pred1)
mse_train2 = mean_squared_error(y_train2, y_train_pred2)
mse_test2  = mean_squared_error(y_test2,  y_pred2)
mse_train3 = mean_squared_error(y_train3, y_train_pred3)
mse_test3  = mean_squared_error(y_test3,  y_pred3)
mse_train4 = mean_squared_error(y_train4, y_train_pred4)
mse_test4  = mean_squared_error(y_test4,  y_pred4)

train_mse = np.array([mse_train1, mse_train2, mse_train3, mse_train4], dtype=float)
test_mse  = np.array([mse_test1,  mse_test2,  mse_test3,  mse_test4 ], dtype=float)

order = np.argsort(num_features)
x = num_features[order]
y_train = train_mse[order]
y_test  = test_mse[order]
```

```{python}
train_curve = np.poly1d(np.polyfit(x, y_train, 2))
test_curve  = np.poly1d(np.polyfit(x, y_test,  2))

plt.plot(x, y_train, color="gray")
plt.plot(x, y_test,  color="red")

plt.scatter(x, y_train, color="skyblue")
plt.scatter(x, y_test,  color="orange")

plt.legend()
plt.show()
```

The model with every variable except bill_depth was the best and even had a better MSE in test than train
